{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook para asignar los embeddings de las respuestas de acuerdo con la implementación de GloVe preentrenado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cargar librerias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\angel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\angel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar el embedding preentrenado (GloVe en espanol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe Model...\n",
      "Done. 855380 words loaded!\n"
     ]
    }
   ],
   "source": [
    "def load_glove_model(glove_file):\n",
    "    print(\"Loading GloVe Model...\")\n",
    "    glove_model = {}\n",
    "    with open(glove_file, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            embedding = np.array([float(val) for val in split_line[1:]])\n",
    "            glove_model[word] = embedding\n",
    "    print(\"Done.\", len(glove_model), \"words loaded!\")\n",
    "    return glove_model\n",
    "\n",
    "glove_file = 'glove_6B/glove-sbwc.i25.vec'\n",
    "glove_model = load_glove_model(glove_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar dataset \n",
    "\n",
    "(Solamente hacer si se desea probar otro arreglo aleatorio diferente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data= pd.read_csv('datos/train_poll_v1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mezclar los valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scramble the values of the data frame and reset the index\n",
    "#data = data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "#data.to_csv('datos/train_poll_v1s1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generar los vectores GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizar las palabras de cada una de las oraciones de cada respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv('datos/train_poll_v1s1.csv')\n",
    "respuestas = list(data['respuestas'].values)\n",
    "#counter=0 Este counter se utiliza si se desea limitar el número de oraciones en cada respuesta\n",
    "indice_extras=[]\n",
    "respuestas_token = []\n",
    "for i in respuestas:\n",
    "    #Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(i)\n",
    "    #if len(sentences)<3 or len(sentences)>100:\n",
    "     #   indice_extras.append(counter)\n",
    "    #else:\n",
    "        # Tokenize each sentence into words\n",
    "    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "    respuestas_token.append(tokenized_sentences)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo de tokenizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Tener',\n",
       "  'la',\n",
       "  'disciplina',\n",
       "  'y',\n",
       "  'constancia',\n",
       "  'que',\n",
       "  'se',\n",
       "  'necesita',\n",
       "  'para',\n",
       "  'tener',\n",
       "  'un',\n",
       "  'buen',\n",
       "  'desempeño',\n",
       "  'en',\n",
       "  'el',\n",
       "  'salón']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuestas_token[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stopwords\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "#Esta instrucción se puede comentar si no se desea eliminar una palabra repetitiva\n",
    "#stop_words.add(\"(Reuters)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generar un embedding para cada oracion de la respuesta\n",
    "\n",
    "(Haciendo un promedio de los embeddings de cada palabra en una oración)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embedding(sentence_tokens, embeddings, stop_words):\n",
    "    embedding_dim = 300  # GloVe 50d\n",
    "    sentence_vector = np.zeros(embedding_dim)\n",
    "    word_count = 0\n",
    "    \n",
    "    for word in sentence_tokens:\n",
    "        word = word.lower()\n",
    "        if word not in stop_words and word in embeddings:\n",
    "            sentence_vector += embeddings[word]\n",
    "            word_count += 1\n",
    "            \n",
    "    if word_count > 0:\n",
    "        sentence_vector /= word_count  # Optionally, normalize by number of words\n",
    "    \n",
    "    return sentence_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate embeddings for each sentence, ignoring stopwords\n",
    "gloVe_Embedding=[]\n",
    "for tokenized_sentences in respuestas_token:\n",
    "\n",
    "    sentence_embeddings = [sentence_embedding(sentence, glove_model, stop_words) for sentence in tokenized_sentences]\n",
    "    sentence_embeddings = np.array(sentence_embeddings)\n",
    "    gloVe_Embedding.append(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the values\n",
    "np.save('datos/encuestav2_gloVe_Values.npy', gloVe_Embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcwtdaENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
